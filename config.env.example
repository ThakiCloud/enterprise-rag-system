# Model Provider Configuration
# Supported providers: openai, anthropic, google, ollama, lm-studio, vllm, custom
MODEL_PROVIDER=openai

# OpenAI Configuration
OPENAI_API_KEY=sk-your-openai-api-key-here
OPENAI_MODEL_NAME=gpt-4o

# Anthropic (Claude) Configuration  
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here
ANTHROPIC_MODEL_NAME=claude-3-opus-20240229

# Google (Gemini) Configuration
GOOGLE_API_KEY=your-google-api-key-here
GOOGLE_MODEL_NAME=gemini-1.5-pro-latest

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL_NAME=llama3

# LM Studio Configuration
LM_STUDIO_BASE_URL=http://localhost:1234/v1

# vLLM / Custom OpenAI-compatible APIs
CUSTOM_API_BASE_URL=http://localhost:8000/v1
CUSTOM_API_KEY=not-needed
CUSTOM_MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.2

# Backend Configuration
BACKEND_URL=http://127.0.0.1:8000

# UI Configuration (for docker-compose)
UI_PORT=8501
BACKEND_PORT=8000

# Memory System Configuration
# Enable/disable memory system by default
ENABLE_MEMORY_SYSTEM=true

# Memory database path (SQLite)
MEMORY_DB_PATH=tmp/session_memories.db

# Default memory settings for queries
USE_MEMORY_DEFAULT=true
MAX_MEMORY_CONTEXT=3
MAX_HISTORY_MESSAGES=5

# Memory model configuration (uses same provider as main model by default)
# Set to override with different model for memory processing
# MEMORY_MODEL_PROVIDER=openai
# MEMORY_MODEL_NAME=gpt-3.5-turbo 